{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363ad816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb9e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we need NMS (to use only selected bounding box)\n",
    "# kalman filters\n",
    "# tracker\n",
    "# model for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a72abd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\srihari_mohan\\\\Anaconda3\\\\envs\\\\deepsort\\\\lib\\\\site-packages\\\\ipykernel_launcher.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from absl import flags\n",
    "import sys\n",
    "tf.compat.v1.flags.DEFINE_string('f','','')\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd80a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.detection import Detection\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.tracker import Tracker\n",
    "from deep_sort.preprocessing import non_max_suppression\n",
    "from tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d670c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov3_tf2.dataset import transform_images\n",
    "from yolov3_tf2.utils import load_darknet_weights, convert_boxes\n",
    "from yolov3_tf2.models import YoloV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5a8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/labels/coco.names','rb') as f:\n",
    "    class_names = f.read().decode()\n",
    "    class_names = class_names.split(\"\\r\\n\")\n",
    "    class_names = class_names[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5de172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2a1888d6a30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YOLO using the weights\n",
    "# load the model file pb mars-small128.pb (feature extractor from bounding box)\n",
    "# Need to evaluate features so needed some distance metric for similarity\n",
    "yolo = YoloV3(classes=len(class_names))\n",
    "yolo.load_weights(\"weights/yolov3.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e745c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = r\"model_data/mars-small128.pb\"\n",
    "encoder = gdet.create_box_encoder(feature_model)\n",
    "# Define a metric (cosine)\n",
    "cosine_threshold = 0.5\n",
    "nn_budget = None\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", cosine_threshold, budget=nn_budget)\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5544a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now yolo detection done\n",
    "# encoder extracts features\n",
    "# tracker compares the features between t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866357c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('data/video/cars.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results.avi', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "from _collections import deque\n",
    "pts = [deque(maxlen=30) for _ in range(1000)]\n",
    "\n",
    "counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1f067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "_, img = vid.read()\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b55bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2baa88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a4f002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1080, 1920, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f965e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transform_images(img, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93168feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 416, 416, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ac3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box, score, classes, nums = yolo.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38700fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba389c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 2., 7., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d671b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for i in range(len(classes)):\n",
    "    names.append(class_names[int(classes[i])])\n",
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dcf5d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'person',\n",
       "       'person', 'car', 'truck', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person',\n",
       "       'person', 'person', 'person', 'person', 'person', 'person'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d44657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in each box and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79f597f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_box = convert_boxes(img, box[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee4b7778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to extract image patch: [169, 0, 63, 0].\n",
      "WARNING: Failed to extract image patch: [257, 0, 69, 0].\n",
      "WARNING: Failed to extract image patch: [135, 0, 59, 0].\n",
      "WARNING: Failed to extract image patch: [193, 0, 49, 0].\n",
      "WARNING: Failed to extract image patch: [178, 0, 85, 0].\n",
      "WARNING: Failed to extract image patch: [62, 0, 23, 0].\n",
      "WARNING: Failed to extract image patch: [125, 0, 54, 0].\n",
      "WARNING: Failed to extract image patch: [90, 0, 17, 0].\n",
      "WARNING: Failed to extract image patch: [276, 0, 12, 0].\n",
      "WARNING: Failed to extract image patch: [181, 0, 15, 0].\n",
      "WARNING: Failed to extract image patch: [105, 0, 40, 0].\n",
      "WARNING: Failed to extract image patch: [105, 0, 40, 0].\n"
     ]
    }
   ],
   "source": [
    "features = encoder(img, converted_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "381d1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                zip(converted_box, score[0], names, features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1621df",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxs = np.array([d.tlwh for d in detections])\n",
    "scores = np.array([d.confidence for d in detections])\n",
    "classes = np.array([d.class_name for d in detections])\n",
    "nms_max_overlap = 0.8\n",
    "indices = non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "detections = [detections[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd3b0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srihari_mohan\\Documents\\PersonalProjects\\DeepSortTracking\\Single-Multiple-Custom-Object-Detection-and-Tracking-master\\deep_sort\\detection.py:54: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ret[2] /= ret[3]\n"
     ]
    }
   ],
   "source": [
    "tracker.predict()\n",
    "tracker.update(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "908febc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deep_sort.tracker.Tracker at 0x2a1851b6490>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da7bd7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n",
      "[nan  0. nan  0.]\n"
     ]
    }
   ],
   "source": [
    "for track in tracker.tracks:\n",
    "    bbox = track.to_tlbr()\n",
    "    class_name= track.get_class()\n",
    "    print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87cd73bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c97cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsort",
   "language": "python",
   "name": "deepsort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
